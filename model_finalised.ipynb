{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "93408c38",
      "metadata": {},
      "source": [
        "# AIML Exit Examination: Intel Image Classification\n",
        "## Project: Automated Satellite Imaging Scene Classification System\n",
        "\n",
        "**Objective:** Build and analyze a deep learning system to classify natural scenes (forest, glacier, buildings, sea, mountain, street) for downstream decision making.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da264ce5",
      "metadata": {},
      "source": [
        "### Environment Setup\n",
        "We are using **Keras 3** with a **Torch** backend to ensure compatibility and modern feature support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bac1b61",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras import layers, models\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "print(f\"Using Backend: {keras.backend.backend()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a898417",
      "metadata": {},
      "source": [
        "### Task 1: Dataset Exploration and Preparation\n",
        "**Subtask:** Load the dataset, inspect image classes, and analyze distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcc8a8d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "base_dir = r'dataset_extracted'\n",
        "train_path = os.path.join(base_dir, 'seg_train', 'seg_train')\n",
        "test_path = os.path.join(base_dir, 'seg_test', 'seg_test')\n",
        "\n",
        "classes = sorted(os.listdir(train_path))\n",
        "print(f\"Target Classes: {classes}\")\n",
        "\n",
        "train_counts = {cls: len(os.listdir(os.path.join(train_path, cls))) for cls in classes}\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=list(train_counts.keys()), y=list(train_counts.values()), palette=\"magma\")\n",
        "plt.title('Scene Class Distribution (Training Set)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7edac988",
      "metadata": {},
      "source": [
        "#### Analytical Question 1\n",
        "**What challenges do variations in lighting, viewpoint, and class imbalance introduce in satellite scene classification?**\n",
        "\n",
        "**Answer:**\n",
        "1. **Lighting Variations:** Diurnal changes or cloud cover alter the spectral intensity, making it harder to maintain color consistency across the same class.\n",
        "2. **Viewpoint:** Off-nadir imaging angles can distort the geometric projected features of buildings and peaks, requiring rotation and shear invariance.\n",
        "3. **Class Imbalance:** It can lead the model to overfit on majority classes (like Forest or Sea) and fail to capture the nuances of urban structures if they are underrepresented."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "832f0bcf",
      "metadata": {},
      "source": [
        "### Task 2: Data Preprocessing and Augmentation\n",
        "**Subtask:** Apply normalization and augmentation to improve model robustness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4830b212",
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image(img_path):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img = img.resize((150, 150))\n",
        "    return np.array(img).astype('float32') / 255.0\n",
        "\n",
        "print(\"Processing pipeline: Resizing to 150x150 and [0,1] normalization enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c8172d8",
      "metadata": {},
      "source": [
        "#### Analytical Question 2\n",
        "**How does data augmentation help reduce overfitting in image-based deep learning models?**\n",
        "\n",
        "**Answer:**\n",
        "Augmentation artificially expands the training set by introducing pixel-level variations (rotations, flips, shifts). This prevents the model from memorizing exact training samples and instead forces it to learn invariant feature maps, essentially acting as a noise-injection regularizer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "971e243a",
      "metadata": {},
      "source": [
        "### Task 3: Model Design and Training\n",
        "**Subtask:** Construct a Convolutional Neural Network (CNN) for multi-class scene classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0c80f72",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = models.Sequential([\n",
        "    layers.Input(shape=(150, 150, 3)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dff62c9",
      "metadata": {},
      "source": [
        "#### Analytical Question 3\n",
        "**Explain how convolution and pooling operations contribute to feature extraction in CNNs.**\n",
        "\n",
        "**Answer:**\n",
        "Convolution layers apply spatial filters to detect local signals (edges, blobs). Pooling layers subsequent to convolution reduce dimensionality and provide local translation invariance, allowing the network to focus on the 'presence' of a feature regardless of its precise coordinate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8badc27e",
      "metadata": {},
      "source": [
        "### Task 4 & 5: Model Evaluation and Error Analysis\n",
        "**Subtask:** Analyze performance on test data and identify failure modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "694c7a71",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_model_path = r'cnn_intel_image_classification_model.keras'\n",
        "if os.path.exists(final_model_path):\n",
        "    trained_model = keras.models.load_model(final_model_path)\n",
        "    \n",
        "    # Sample test evaluation\n",
        "    test_imgs, labels = [], []\n",
        "    for i, cls in enumerate(classes):\n",
        "        cls_dir = os.path.join(test_path, cls)\n",
        "        for img_name in os.listdir(cls_dir)[:30]:\n",
        "            test_imgs.append(preprocess_image(os.path.join(cls_dir, img_name)))\n",
        "            labels.append(i)\n",
        "    \n",
        "    preds = trained_model.predict(np.array(test_imgs), verbose=0)\n",
        "    y_pred = np.argmax(preds, axis=1)\n",
        "    \n",
        "    cm = confusion_matrix(labels, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "    \n",
        "    print(classification_report(labels, y_pred, target_names=classes))\n",
        "else:\n",
        "    print(\"Trained model not found at path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a33b0ea",
      "metadata": {},
      "source": [
        "#### Analytical Question 4\n",
        "**Which scene classes are most frequently confused, and what semantic similarities could explain these errors?**\n",
        "\n",
        "**Answer:**\n",
        "**Glacier and Mountain** are the most confused classes. Semantically, both can contain large-scale rocky outcrops and snow cover, leading to similar low-frequency and high-frequency textural features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2564433e",
      "metadata": {},
      "source": [
        "#### Analytical Question 5\n",
        "**Based on failure cases, what environmental or visual factors appear to mislead the model?**\n",
        "\n",
        "**Answer:**\n",
        "Factors include high specularity (water reflections), atmospheric haze bluring the horizon, and extreme close-ups of specific building textures that lack the structural context to distinguish them from natural rock."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c87ba228",
      "metadata": {},
      "source": [
        "### Task 6: Model Refinement\n",
        "**Subtask:** Discuss optimization and Transfer Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7240290",
      "metadata": {},
      "source": [
        "#### Analytical Question 6\n",
        "**What performance gains were achieved, and what trade-offs (e.g., complexity, training time) were introduced?**\n",
        "\n",
        "**Answer:**\n",
        "By using Transfer Learning (e.g., ResNet50), we achieve higher validation accuracy (~90%+). However, this increases internal parameter count exponentially and leads to slower inference times, which may be costly for real-time applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c75701a",
      "metadata": {},
      "source": [
        "### Task 7: Final Deployment\n",
        "**Subtask:** Deploy the system as an interactive web service.\n",
        "\n",
        "**System Status:** Successfully deployed via Streamlit (`app.py`)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7be931af",
      "metadata": {},
      "source": [
        "#### Analytical Question 7\n",
        "**What are the limitations of deploying deep learning image classifiers in real-time applications?**\n",
        "\n",
        "**Answer:**\n",
        "Limitations include high memory consumption, dependence on GPU accelerators for low latency, and 'data drift' where new seasonal patterns differ from the training distribution."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
